@article{alg:rdg,
  title     = {A recursive decomposition method for large scale continuous optimization},
  author    = {Sun, Yuan and Kirley, Michael and Halgamuge, Saman K},
  journal   = {IEEE Transactions on Evolutionary Computation},
  volume    = {22},
  number    = {5},
  pages     = {647--661},
  year      = {2018},
  publisher = {IEEE},
  keywords  = {Optimization;Benchmark testing;Australia;Electronic mail;Iron;Particle swarm optimization;Continuous optimization problem;cooperative co-evolution (CC);decomposition method;large scale global optimization (LSGO)},
  doi       = {10.1109/TEVC.2017.2778089}
}

@inproceedings{alg:rdg2,
  author    = {Sun, Yuan and Omidvar, Mohammad Nabi and Kirley, Michael and Li, Xiaodong},
  title     = {Adaptive threshold parameter estimation with recursive differential grouping for problem decomposition},
  year      = {2018},
  isbn      = {9781450356183},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3205455.3205483},
  doi       = {10.1145/3205455.3205483},
  abstract  = {Problem decomposition plays an essential role in the success of cooperative co-evolution (CC), when used for solving large-scale optimization problems. The recently proposed recursive differential grouping (RDG) method has been shown to be very efficient, especially in terms of time complexity. However, it requires an appropriate parameter setting to estimate a threshold value in order to determine if two subsets of decision variables interact or not. Furthermore, using one global threshold value may be insufficient to identify variable interactions in components with different contribution to the fitness value. Inspired by the different grouping 2 (DG2) method, in this paper, we adaptively estimates a threshold value based on computational round-off errors for RDG. We derive an upper bound of the round-off errors, which is shown to be sufficient when identifying variable interactions across a wide range of large-scale benchmark problems. Comprehensive numerical experimental results showed that the proposed RDG2 method achieved higher decomposition accuracy than RDG and DG2. When embedded into a CC framework, it achieved statistically equal or significantly better solution quality than RDG and DG2, when used to solve the benchmark problems.},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages     = {889–896},
  numpages  = {8},
  keywords  = {cooperative co-evolution, large-scale continuous optimization, parameter adaptation, problem decomposition, round-off error analysis},
  location  = {Kyoto, Japan},
  series    = {GECCO '18}
}

@inproceedings{alg:rdg3,
  title        = {Decomposition for large-scale optimization problems with overlapping components},
  author       = {Sun, Yuan and Li, Xiaodong and Ernst, Andreas and Omidvar, Mohammad Nabi},
  booktitle    = {2019 IEEE congress on evolutionary computation (CEC)},
  pages        = {326--333},
  year         = {2019},
  organization = {IEEE},
  keywords     = {Optimization;Couplings;Upper bound;Benchmark testing;Australia;Sun;Iron;Cooperative co-evolution;large-scale continuous optimization;overlapping problem;variable interaction;problem decomposition.},
  doi          = {10.1109/CEC.2019.8790204}
}
  
@article{alg:erdg,
  author   = {Yang, Ming and Zhou, Aimin and Li, Changhe and Yao, Xin},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {An Efficient Recursive Differential Grouping for Large-Scale Continuous Problems},
  year     = {2021},
  volume   = {25},
  number   = {1},
  pages    = {159-171},
  keywords = {Optimization;Computational efficiency;Geology;Computer science;Electronic mail;Computational complexity;Automation;Cooperative co-evolution (CC);decomposition;large-scale global optimization},
  doi      = {10.1109/TEVC.2020.3009390}
}
  
@article{alg:trdg,
  author   = {Xu, Hong-Bin and Li, Fei and Shen, Hao},
  journal  = {IEEE Access},
  title    = {A Three-Level Recursive Differential Grouping Method for Large-Scale Continuous Optimization},
  year     = {2020},
  volume   = {8},
  number   = {},
  pages    = {141946-141957},
  keywords = {Optimization;Iron;Sociology;Covariance matrices;Linear programming;Power electronics;Large-scale continuous optimization;cooperative co-evolution (CC);differential grouping;trichotomy method},
  doi      = {10.1109/ACCESS.2020.3013661}
}

@article{alg:abc,
  title     = {A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm},
  author    = {Karaboga, Dervis and Basturk, Bahriye},
  journal   = {Journal of global optimization},
  volume    = {39},
  pages     = {459--471},
  year      = {2007},
  publisher = {Springer},
  doi       = {10.1007/s10898-007-9149-x}
}

@article{alg:aco,
  title   = {ACO algorithms for the traveling salesman problem},
  author  = {St{\"u}tzle, Thomas and Dorigo, Marco and others},
  journal = {Evolutionary algorithms in engineering and computer science},
  volume  = {4},
  pages   = {163--183},
  year    = {1999}
}

@article{alg:dg,
  title     = {Cooperative co-evolution with differential grouping for large scale optimization},
  author    = {Omidvar, Mohammad Nabi and Li, Xiaodong and Mei, Yi and Yao, Xin},
  journal   = {IEEE Transactions on evolutionary computation},
  volume    = {18},
  number    = {3},
  pages     = {378--393},
  year      = {2013},
  publisher = {IEEE},
  keywords  = {Optimization;Couplings;Linear programming;Evolutionary computation;Genetic algorithms;Context;Vectors;cooperative co-evolution;large-scale optimization;problem decomposition;non-separability;numerical optimization;Cooperative co-evolution;large-scale optimization;nonseparability;numerical optimization;problem decomposition},
  doi       = {10.1109/TEVC.2013.2281543}
}

@article{alg:ddg,
  title     = {Dual differential grouping: A more general decomposition method for large-scale optimization},
  author    = {Li, Jian-Yu and Zhan, Zhi-Hui and Tan, Kay Chen and Zhang, Jun},
  journal   = {IEEE Transactions on Cybernetics},
  year      = {2023},
  publisher = {IEEE},
  volume    = {53},
  number    = {6},
  pages     = {3624-3638},
  keywords  = {Optimization;Artificial neural networks;Time complexity;Research and development;Problem-solving;Minimization;Linear programming;Cooperative coevolution (CC);differential evolution;dual differential grouping (DDG);evolutionary computation (EC);large-scale optimization problem (LSOP);particle swarm optimization},
  doi       = {10.1109/TCYB.2022.3158391}
}

@article{alg:dg2,
  title     = {DG2: A faster and more accurate differential grouping for large-scale black-box optimization},
  author    = {Omidvar, Mohammad Nabi and Yang, Ming and Mei, Yi and Li, Xiaodong and Yao, Xin},
  journal   = {IEEE Transactions on Evolutionary Computation},
  volume    = {21},
  number    = {6},
  pages     = {929--942},
  year      = {2017},
  publisher = {IEEE},
  keywords  = {Optimization;Linear programming;Benchmark testing;Sensitivity;Roundoff errors;Computer science;Reliability;Cooperative co-evolution;differential grouping (DG);large-scale global optimization;problem decomposition},
  doi       = {10.1109/TEVC.2017.2694221}
}

@article{alg:fii,
  title    = {Cooperation coevolution with fast interdependency identification for large scale optimization},
  journal  = {Information Sciences},
  volume   = {381},
  pages    = {142-160},
  year     = {2017},
  issn     = {0020-0255},
  doi      = {10.1016/j.ins.2016.11.013},
  url      = {https://www.sciencedirect.com/science/article/pii/S002002551631800X},
  author   = {Xiao-Min Hu and Fei-Long He and Wei-Neng Chen and Jun Zhang},
  keywords = {Cooperative coevolution (CC), Large scale global optimization (LSGO), Problem decomposition, Differential evolution},
  abstract = {Cooperative coevolution (CC) provides a powerful divide-and-conquer architecture for large scale global optimization (LSGO). However, its performance relies highly on decomposition. To make near-optimal decomposition, most developed decomposition strategies either cannot obtain the correct interdependency information or require a lot of fitness evaluations (FEs) in the identification. To alleviate the limitations in previous works, in this paper we propose a fast interdependency identification (FII) algorithm for CC in LSGO. The proposed algorithm firstly identifies separable and nonseparable variables efficiently. Then, the interdependency information of nonseparable variables is further investigated. To make near-optimal decomposition for CC, our algorithm avoids the necessity of obtaining the full interdependency information of nonseparable variables. Therefore, a significant number of FEs can be saved. Extensive experiments have been conducted on two suites of LSGO benchmark functions with up to 2000 variables. FII correctly identified the interdependency information on most benchmark functions with much fewer FEs in comparison with three state-of-the-art algorithms. Furthermore, combined with CC and coupled with a differential evolution variant serving as the optimizer, FII has shown its promising performance in LSGO.}
}

@article{alg:edg,
  author   = {Kumar, Abhishek and Das, Swagatam and Mallipeddi, Rammohan},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {An Efficient Differential Grouping Algorithm for Large-Scale Global Optimization},
  year     = {2024},
  volume   = {28},
  number   = {1},
  pages    = {32-46},
  keywords = {Optimization;Iron;Matrix decomposition;Covariance matrices;Computational efficiency;Search problems;Time complexity;Cooperative co-evolution (CC);covariance matrix adaptation evolution strategy (CMA-ES);efficient differential grouping (DG);large-scale optimization problems;matrix decomposition},
  doi      = {10.1109/TEVC.2022.3230070}
}

@inproceedings{alg:xdg,
  author    = {Sun, Yuan and Kirley, Michael and Halgamuge, Saman Kumara},
  title     = {Extended Differential Grouping for Large Scale Global Optimization with Direct and Indirect Variable Interactions},
  year      = {2015},
  isbn      = {9781450334723},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2739480.2754666},
  doi       = {10.1145/2739480.2754666},
  abstract  = {Cooperative co-evolution is a framework that can be used to effectively solve large scale optimization problems. This approach employs a divide and conquer strategy, which decomposes the problem into sub-components that are optimized separately. However, solution quality relies heavily on the decomposition method used. Ideally, the interacting decision variables should be assigned to the same sub-component and the interdependency between sub-components should be kept to a minimum. Differential grouping, a recently proposed method, has high decomposition accuracy across a suite of benchmark functions. However, we show that differential grouping can only identify decision variables that interact directly. Subsequently, we propose an extension of differential grouping that is able to correctly identify decision variables that also interact indirectly. Empirical studies show that our extended differential grouping method achieves perfect decomposition on all of the benchmark functions investigated. Significantly, when our decomposition method is embedded in the cooperative co-evolution framework, it achieves comparable or better solution quality than the differential grouping method.},
  booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  pages     = {313–320},
  numpages  = {8},
  keywords  = {variable interaction, problem decomposition, large scale global optimization, cooperative co-evolution},
  location  = {Madrid, Spain},
  series    = {GECCO '15}
}

@article{alg:gdg,
  author     = {Mei, Yi and Omidvar, Mohammad Nabi and Li, Xiaodong and Yao, Xin},
  title      = {A Competitive Divide-and-Conquer Algorithm for Unconstrained Large-Scale Black-Box Optimization},
  year       = {2016},
  issue_date = {June 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {42},
  number     = {2},
  issn       = {0098-3500},
  url        = {https://doi.org/10.1145/2791291},
  doi        = {10.1145/2791291},
  abstract   = {This article proposes a competitive divide-and-conquer algorithm for solving large-scale black-box optimization problems for which there are thousands of decision variables and the algebraic models of the problems are unavailable. We focus on problems that are partially additively separable, since this type of problem can be further decomposed into a number of smaller independent subproblems. The proposed algorithm addresses two important issues in solving large-scale black-box optimization: (1) the identification of the independent subproblems without explicitly knowing the formula of the objective function and (2) the optimization of the identified black-box subproblems. First, a Global Differential Grouping (GDG) method is proposed to identify the independent subproblems. Then, a variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is adopted to solve the subproblems resulting from its rotation invariance property. GDG and CMA-ES work together under the cooperative co-evolution framework. The resultant algorithm, named CC-GDG-CMAES, is then evaluated on the CEC’2010 large-scale global optimization (LSGO) benchmark functions, which have a thousand decision variables and black-box objective functions. The experimental results show that, on most test functions evaluated in this study, GDG manages to obtain an ideal partition of the index set of the decision variables, and CC-GDG-CMAES outperforms the state-of-the-art results. Moreover, the competitive performance of the well-known CMA-ES is extended from low-dimensional to high-dimensional black-box problems.},
  journal    = {ACM Trans. Math. Softw.},
  month      = {jun},
  articleno  = {13},
  numpages   = {24},
  keywords   = {Large-scale black-box optimization, cooperative co-evolution, covariance matrix adaptation evolutionary strategy (CMA-ES), decomposition, differential grouping}
}
  
@inproceedings{alg:pso,
  author    = {Kennedy, James and Eberhart, Russell},
  year      = {1995},
  month     = {12},
  pages     = {1942 - 1948 vol.4},
  title     = {Particle swarm Optimization},
  volume    = {4},
  isbn      = {0-7803-2768-3},
  journal   = {Proceedings of IEEE International Conference on Neural Networks},
  booktitle = {Proceedings of IEEE International Conference on Neural Networks},
  doi       = {10.1109/ICNN.1995.488968}
}

@inproceedings{alg:ldwpso,
  author    = {Shi, Y. and Eberhart, R.},
  booktitle = {1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)},
  title     = {A modified particle swarm optimizer},
  year      = {1998},
  volume    = {},
  number    = {},
  pages     = {69-73},
  keywords  = {Particle swarm optimization;Nonlinear equations;Computational modeling;Evolutionary computation;Genetic programming;Genetic algorithms;Genetic mutations;Educational institutions;Birds;Collaboration},
  doi       = {10.1109/ICEC.1998.699146}
}

@article{alg:cpso,
  title    = {Center particle swarm optimization},
  journal  = {Neurocomputing},
  volume   = {70},
  number   = {4},
  pages    = {672-679},
  year     = {2007},
  note     = {Advanced Neurocomputing Theory and Methodology},
  issn     = {0925-2312},
  doi      = {10.1016/j.neucom.2006.10.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231206002700},
  author   = {Yu Liu and Zheng Qin and Zhewen Shi and Jiang Lu},
  keywords = {Particle swarm optimization, Neural networks, Evolutionary computation},
  abstract = {Center particle swarm optimization algorithm (CenterPSO) is proposed where a center particle is incorporated into linearly decreasing weight particle swarm optimization (LDWPSO). Unlike other ordinary particles in LDWPSO, the center particle has no explicit velocity, and is set to the center of the swarm at every iteration. Other aspects of the center particle are the same as that of the ordinary particle, such as fitness evaluation and competition for the best particle of the swarm. Because the center of the swarm is a promising position, the center particle generally gets good fitness value. More importantly, due to frequent appearance as the best particle of swarm, it often attracts other particles and guides the search direction of the whole swarm. CenterPSO and LDWPSO are extensively compared on three well-known benchmark functions with 10, 20, 30 dimensions. Experimental results show that CenterPSO achieves not only better solutions but also faster convergence. Furthermore, CenterPSO and LDWPSO are compared as neural network training algorithms. The results show that CenterPSO achieves better performance than LDWPSO.}
}

@inproceedings{alg:mpso,
  author    = {Wang, Hui and Li, Changhe and Liu, Yong and Zeng, Sanyou},
  booktitle = {2007 IEEE Swarm Intelligence Symposium},
  title     = {A Hybrid Particle Swarm Algorithm with Cauchy Mutation},
  year      = {2007},
  volume    = {},
  number    = {},
  pages     = {356-360},
  keywords  = {Particle swarm optimization;Genetic mutations;Evolutionary computation;Computer science;Geology;Search problems;Random number generation;Velocity control;Genetic programming;Testing},
  doi       = {10.1109/SIS.2007.367959}
}

@article{alg:mcupso,
  title    = {Unified particle swarm delivers high efficiency to particle swarm optimization},
  journal  = {Applied Soft Computing},
  volume   = {55},
  pages    = {371 -- 383},
  year     = {2017},
  issn     = {1568-4946},
  doi      = {10.1016/j.asoc.2017.02.008},
  url      = {http://www.sciencedirect.com/science/article/pii/S1568494617300765},
  author   = {Hsing-Chih Tsai},
  keywords = {Particle swarm optimization, Unified particle swarm, Parameter selection},
  abstract = {This paper suggests integrating a unification factor into particle swarm optimization (PSO) to balance the effects of cognitive and social terms. The resultant unified particle swarm (UPS) moves particles toward the center of its personal best and the global best. This improves on PSO, which moves particles far beyond the center. Widely used benchmark functions and four types of experiments demonstrate that the proposed UPS uses slightly more computational time than PSO to attain significantly higher efficiency and, usually, better solution effectiveness and consistency than PSO. Robust performance was further demonstrated by the significantly higher efficiency and better solution effectiveness and stability achieved by the UPS, as compared to the PSO and its variants. Outstandingly, convergence speeds for the proposed UPS were very good on the 13 benchmark functions examined in experiment 1, demonstrating the correct movement of UPS particles toward convergence.}
},

@article{alg:clpso,
  author   = {J. J. {Liang} and A. K. {Qin} and P. N. {Suganthan} and S. {Baskar}},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {Comprehensive learning particle swarm optimizer for global optimization of multimodal functions},
  year     = {2006},
  volume   = {10},
  number   = {3},
  pages    = {281-295},
  keywords = {learning (artificial intelligence);particle swarm optimisation;comprehensive learning particle swarm optimizer;global optimization;multimodal functions;particle historical best information;particle velocity;premature convergence;Rosenbrock multimodal test function;Griewank multimodal test function;Rastrigin multimodal test function;Ackley multimodal test function;Schwefel multimodal test function;composition functions;coordinate rotation;multimodal problems;Particle swarm optimization;Space technology;Acceleration;Convergence;Benchmark testing;Evolutionary computation;Genetic mutations;Insects;Animals;Birds;Composition benchmark functions;comprehensive learning particle swarm optimizer (CLPSO);global numerical optimization;particle swarm optimizer (PSO)},
  doi      = {10.1109/TEVC.2005.857610},
  issn     = {1089-778X}
},

@incollection{alg:ovcpso,
  author    = {Shahzad, Farrukh and Baig, A. Rauf and Masood, Sohail and Kamran, Muhammad and Naveed, Nawazish},
  editor    = {Yu, Wen and Sanchez, Edgar N.},
  title     = {Opposition-Based Particle Swarm Optimization with Velocity Clamping (OVCPSO)},
  booktitle = {Advances in Computational Intelligence},
  year      = {2009},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {339--348},
  abstract  = {This paper presents an Opposition-based PSO(OVCPSO) which uses Velocity Clamping to accelerate its convergence speed and to avoid premature convergence of algorithm. Probabilistic opposition-based learning for particles has been used in the proposed method which uses velocity clamping to control the speed and direction of particles. Experiments have been performed upon various well known benchmark optimization problems and results have shown that OVCPSO can deal with difficult unimodal and multimodal optimization problems efficiently and effectively. The numbers of function calls (NFC) are significantly less than other PSO variants i.e. basic PSO with inertia weight, PSO with inertia weight and velocity clamping (VCPSO) and opposition based PSO with Cauchy Mutation (OPSOCM).},
  doi       = {10.1007/978-3-642-03156-4_34},
  isbn      = {978-3-642-03156-4}
}

@inproceedings{alg:npscc,
  author    = {Aote, Shailendra S. and Raghuwanshi, M. M. and Malik, L. G.},
  editor    = {Satapathy, Suresh Chandra and Biswal, Bhabendra Narayan and Udgata, Siba K. and Mandal, J.K.},
  title     = {A New Particle Swarm Optimizer with Cooperative Coevolution for Large Scale Optimization},
  booktitle = {Proceedings of the 3rd International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2014},
  year      = {2015},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {781--789},
  abstract  = {With the increasing demands in solving larger dimensional problems, it is necessary to have efficient algorithm. Efforts were put towards increasing the efficiency of the algorithms. This paper presents a new approach of particle swarm optimization with cooperative coevolution. The proposed technique [NPSO-CC] is built on the success of an early CCPSO2 that employs an effective variable grouping technique random grouping. The technique of moving away out of the local minima is presented in the paper. Instead of using simple velocity update equation, the new velocity update equation is used from where the contribution of worst particle is subtracted. Experimental results show that our algorithm performs better as compared to other promising techniques on most of the functions.},
  isbn      = {978-3-319-11933-5},
  doi       = {10.1007/978-3-319-11933-5_88}
}

@inproceedings{alg:ccga,
  author    = {Potter, Mitchell A.
               and De Jong, Kenneth A.},
  editor    = {Davidor, Yuval
               and Schwefel, Hans-Paul
               and M{\"a}nner, Reinhard},
  title     = {A cooperative coevolutionary approach to function optimization},
  booktitle = {Parallel Problem Solving from Nature --- PPSN III},
  year      = {1994},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {249--257},
  abstract  = {A general model for the coevolution of cooperating species is presented. This model is instantiated and tested in the domain of function optimization, and compared with a traditional GA-based function optimizer. The results are encouraging in two respects. They suggest ways in which the performance of GA and other EA-based optimizers can be improved, and they suggest a new approach to evolving complex structures such as neural networks and rule sets.},
  isbn      = {978-3-540-49001-2}
}
  
@article{bech:cec2010lsgo,
  title   = {Benchmark functions for the CEC’2010 special session and competition on large-scale global optimization},
  author  = {Tang, Ke and Li, Xiaodong and Suganthan, Ponnuthurai Nagaratnam and Yang, Zhenyu and Weise, Thomas},
  journal = {Nature inspired computation and applications laboratory, USTC, China},
  volume  = {24},
  pages   = {1--18},
  year    = {2007}
}

@article{bech:cec2013lsgo,
  title   = {Benchmark functions for the CEC 2013 special session and competition on large-scale global optimization},
  author  = {Li, Xiaodong and Tang, Ke and Omidvar, Mohammad N and Yang, Zhenyu and Qin, Kai and China, Hefei},
  journal = {gene},
  volume  = {7},
  number  = {33},
  pages   = {8},
  year    = {2013}
}

@article{rev:lsgo_bb_pop_alg_p1,
  author   = {Omidvar, Mohammad Nabi and Li, Xiaodong and Yao, Xin},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {A Review of Population-Based Metaheuristics for Large-Scale Black-Box Global Optimization—Part I},
  year     = {2022},
  volume   = {26},
  number   = {5},
  pages    = {802-822},
  keywords = {Optimization;Statistics;Sociology;Convergence;Approximation algorithms;Particle swarm optimization;Maintenance engineering;Black-box optimization;evolutionary optimization;large-scale global optimization;metaheuristics},
  doi      = {10.1109/TEVC.2021.3130838}
}
  
@article{rev:lsgo_bb_pop_alg_p2,
  author   = {Omidvar, Mohammad Nabi and Li, Xiaodong and Yao, Xin},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {A Review of Population-Based Metaheuristics for Large-Scale Black-Box Global Optimization—Part II},
  year     = {2022},
  volume   = {26},
  number   = {5},
  pages    = {823-843},
  keywords = {Optimization;Statistics;Sociology;Convergence;Approximation algorithms;Particle swarm optimization;Maintenance engineering;Black-box optimization;evolutionary optimization;large-scale global optimization;metaheuristics},
  doi      = {10.1109/TEVC.2021.3130835}
}
  
@article{rev:cc_alg,
  author   = {Ma, Xiaoliang and Li, Xiaodong and Zhang, Qingfu and Tang, Ke and Liang, Zhengping and Xie, Weixin and Zhu, Zexuan},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {A Survey on Cooperative Co-Evolutionary Algorithms},
  year     = {2019},
  volume   = {23},
  number   = {3},
  pages    = {421-441},
  keywords = {Optimization;Genetic algorithms;Resource management;Benchmark testing;Computer science;Google;Perturbation methods;Cooperative co-evolutionary algorithm (CCEA);evolutionary algorithm (EA);genetic algorithm (GA)},
  doi      = {10.1109/TEVC.2018.2868770}
}

@article{stat:art:demsar,
  author     = {Dem\v{s}ar, Janez},
  title      = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  year       = {2006},
  issue_date = {12/1/2006},
  publisher  = {JMLR.org},
  volume     = {7},
  issn       = {1532-4435},
  abstract   = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
  journal    = {J. Mach. Learn. Res.},
  month      = {dec},
  pages      = {1–30},
  numpages   = {30},
  url        = {https://dl.acm.org/doi/10.5555/1248547.1248548}
}

@article{stat:art:derrac,
  title    = {A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms},
  journal  = {Swarm and Evolutionary Computation},
  volume   = {1},
  number   = {1},
  pages    = {3-18},
  year     = {2011},
  issn     = {2210-6502},
  doi      = {10.1016/j.swevo.2011.02.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210650211000034},
  author   = {Joaquín Derrac and Salvador García and Daniel Molina and Francisco Herrera},
  keywords = {Statistical analysis, Nonparametric statistics, Pairwise comparisons, Multiple comparisons, Evolutionary algorithms, Swarm intelligence algorithms},
  abstract = {The interest in nonparametric statistical analysis has grown recently in the field of computational intelligence. In many experimental studies, the lack of the required properties for a proper application of parametric procedures–independence, normality, and homoscedasticity–yields to nonparametric ones the task of performing a rigorous comparison among algorithms. In this paper, we will discuss the basics and give a survey of a complete set of nonparametric procedures developed to perform both pairwise and multiple comparisons, for multi-problem analysis. The test problems of the CEC’2005 special session on real parameter optimization will help to illustrate the use of the tests throughout this tutorial, analyzing the results of a set of well-known evolutionary and swarm intelligence algorithms. This tutorial is concluded with a compilation of considerations and recommendations, which will guide practitioners when using these tests to contrast their experimental results.}
}

@article{stat:met:Friedman,
  author    = {Milton Friedman},
  title     = {The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance},
  journal   = {Journal of the American Statistical Association},
  volume    = {32},
  number    = {200},
  pages     = {675-701},
  year      = {1937},
  publisher = {Taylor & Francis},
  doi       = {10.1080/01621459.1937.10503522},
  url       = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1937.10503522},
  eprint    = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1937.10503522}
}

@phdthesis{stat:met:Nemenyi,
  author   = {NEMENYI,PETER B.},
  year     = {1963},
  title    = {DISTRIBUTION-FREE MULTIPLE COMPARISONS},
  journal  = {ProQuest Dissertations and Theses},
  pages    = {127},
  note     = {Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2023-02-17},
  keywords = {Pure sciences; Mathematics; 0405:Mathematics},
  isbn     = {9781084733008},
  language = {English},
  url      = {https://www.proquest.com/dissertations-theses/distribution-free-multiple-comparisons/docview/302256074/se-2}
}

@incollection{stat:met:Wilcoxon,
  author    = {Wilcoxon, Frank},
  editor    = {Kotz, Samuel and Johnson, Norman L.},
  title     = {Individual Comparisons by Ranking Methods},
  booktitle = {Breakthroughs in Statistics: Methodology and Distribution},
  year      = {1992},
  publisher = {Springer New York},
  address   = {New York, NY},
  pages     = {196--202},
  abstract  = {The comparison of two treatments generally falls into one of the following two categories: (a) we may have a number of replications for each of the two treatments, which are unpaired, or (b) we may have a number of paired comparisons leading to a series of differences, some of which may be positive and some negative. The appropriate methods for testing the significance of the differences of the means in these two cases are described in most of the textbooks on statistical methods.},
  isbn      = {978-1-4612-4380-9},
  doi       = {10.1007/978-1-4612-4380-9_16},
  url       = {https://doi.org/10.1007/978-1-4612-4380-9_16}
}
  
@article{met:nonlinearity,
  author   = {Munetomo, Masaharu and Goldberg, David E.},
  title    = {{Linkage Identification by Non-monotonicity Detection for Overlapping Functions}},
  journal  = {Evolutionary Computation},
  volume   = {7},
  number   = {4},
  pages    = {377-398},
  year     = {1999},
  month    = {12},
  abstract = {{This paper presents the linkage identification by non-monotonicity detection (LIMD) procedure and its extension for overlapping functions by introducing the tightness detection (TD) procedure. The LIMD identifies linkage groups directly by performing order-2 simultaneous perturbations on a pair of loci to detect monotonicity/non-monotonicity of fitness changes. The LIMD can identify linkage groups with at most order of k when it is applied to O(2k) strings. The TD procedure calculates tightness of linkage between a pair of loci based on the linkage groups obtained by the LIMD. By removing loci with weak tightness from linkage groups, correct linkage groups are obtained for overlapping functions, which were considered difficult for linkage identification procedures.}},
  issn     = {1063-6560},
  doi      = {10.1162/evco.1999.7.4.377},
  url      = {https://doi.org/10.1162/evco.1999.7.4.377},
  eprint   = {https://direct.mit.edu/evco/article-pdf/7/4/377/1493138/evco.1999.7.4.377.pdf}
}

@article{api:NiaPy,
  author  = {Vrban{\v{c}}i{\v{c}}, Grega and Brezo{\v{c}}nik, Lucija and Mlakar, Uro{\v{s}} and Fister, Du{\v{s}}an and {Fister Jr.}, Iztok},
  title   = {{NiaPy: Python microframework for building nature-inspired algorithms}},
  journal = {{Journal of Open Source Software}},
  year    = {2018},
  volume  = {3},
  issue   = {23},
  issn    = {2475-9066},
  doi     = {10.21105/joss.00613},
  url     = {https://doi.org/10.21105/joss.00613}
}

@article{api:scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2}
}

@article{api:scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

@article{bib:lsgo:telecommunication,
  author    = {Francisco Luna, Antonio J. Nebro, Enrique Alba and Juan J. Durillo},
  title     = {Solving large-scale real-world telecommunication problems using a grid-based genetic algorithm},
  journal   = {Engineering Optimization},
  volume    = {40},
  number    = {11},
  pages     = {1067--1084},
  year      = {2008},
  publisher = {Taylor & Francis},
  doi       = {10.1080/03052150802294581},
  url       = {https://doi.org/10.1080/03052150802294581},
  eprint    = {https://doi.org/10.1080/03052150802294581}
}

@article{bib:lsgo:engineering,
  title    = {Multi-scale optimization for process systems engineering},
  journal  = {Computers and Chemical Engineering},
  volume   = {60},
  pages    = {17--30},
  year     = {2014},
  issn     = {0098--1354},
  doi      = {10.1016/j.compchemeng.2013.07.009},
  url      = {https://www.sciencedirect.com/science/article/pii/S0098135413002299},
  author   = {Lorenz T. Biegler and Yi-dong Lang and Weijie Lin},
  keywords = {Nonlinear programming, Model reduction, Trust region, Process optimization},
  abstract = {Efficient nonlinear programming (NLP) algorithms and modeling platforms have led to powerful process optimization strategies. Nevertheless, these algorithms are challenged by recent evolution and deployment of multi-scale models (such as molecular dynamics and complex fluid flow) that apply over broad time and length scales. Integrated optimization of these models requires accurate and efficient reduced models (RMs). This study develops a rigorous multi-scale optimization framework that substitutes RMs for complex original detailed models (ODMs) and guarantees convergence to the original optimization problem. Based on trust region concepts this framework leads to three related NLP algorithms for RM-based optimization. The first follows the classical gradient-based trust-region method, the second avoids gradient calculations from the ODM, and the third avoids frequent recourse to ODM evaluations, using the concept of ϵ-exact RMs. We illustrate these algorithms with small examples and discuss RM-based optimization case studies that demonstrate their performance and effectiveness.}
}

@inproceedings{bib:lsog:logistics,
  title  = {Robust and Large Scale Network Optimization in Logistics},
  author = {Alexander T. Richter},
  year   = {2018},
  url    = {https://api.semanticscholar.org/CorpusID:196177195},
  doi    = {10.24355/DBBS.084-201811231152-0}
}

@inproceedings{bib:curse_of_dim,
  title={The curse of dimensionality},
  author={K{\"o}ppen, Mario},
  booktitle={5th online world conference on soft computing in industrial applications (WSC5)},
  volume={1},
  pages={4--8},
  year={2000}
}

@article{bib:COD:numerical_methods,
  author   = {McEneaney, William M.},
  title    = {A Curse-of-Dimensionality-Free Numerical Method for Solution of Certain HJB PDEs},
  journal  = {SIAM Journal on Control and Optimization},
  volume   = {46},
  number   = {4},
  pages    = {1239-1276},
  year     = {2007},
  doi      = {10.1137/040610830},
  url      = {https://doi.org/10.1137/040610830},
  eprint   = {https://doi.org/10.1137/040610830},
  abstract = { In previous works of the author and others, max-plus methods have been explored for the solution of first-order, nonlinear Hamilton–Jacobi–Bellman partial differential equations (HJB PDEs) and corresponding nonlinear control problems. These methods exploit the max-plus linearity of the associated semigroups. In particular, although the problems are nonlinear, the semigroups are linear in the max-plus sense. These methods have been used successfully to compute solutions. Although they provide certain computational-speed advantages, they still generally suffer from the curse of dimensionality. Here we consider HJB PDEs in which the Hamiltonian takes the form of a (pointwise) maximum of linear/quadratic forms. The approach to the solution will be rather general, but in order to ground the work, we consider only constituent Hamiltonians corresponding to long-run average-cost-per-unit-time optimal control problems for the development. We obtain a numerical method not subject to the curse of dimensionality. The method is based on construction of the dual-space semigroup corresponding to the HJB PDE. This dual-space semigroup is constructed from the dual-space semigroups corresponding to the constituent linear/quadratic Hamiltonians. The dual-space semigroup is particularly useful due to its form as a max-plus integral operator with a kernel obtained from the originating semigroup. One considers repeated application of the dual-space semigroup to obtain the solution. }
}

@article{bib:COD:machine_learning,
  title    = {A machine learning approach to circumventing the curse of dimensionality in discontinuous time series machine data},
  journal  = {Reliability Engineering and System Safety},
  volume   = {195},
  pages    = {106706},
  year     = {2020},
  issn     = {0951-8320},
  doi      = {10.1016/j.ress.2019.106706},
  url      = {https://www.sciencedirect.com/science/article/pii/S0951832019304752},
  author   = {Oluseun Omotola Aremu and David Hyland-Wood and Peter Ross McAree},
  keywords = {Machine learning, Deep learning, Dimension reduction, Manifold learning, Predictive maintenance, Prognostics, Partial differential equations},
  abstract = {The growing interest in artificial intelligence has led to current data-driven predictive maintenance (PdM) relying on machine learning (ML) algorithms. Although ML algorithms are useful for data-intensive analysis, research shows that their performance and reliability are reduced when high-dimensional data is used for training and testing. Raw machine data can be high-dimensional due to multi-sensor measurements and discontinuous due to the wide ranges of parameter variations during continuous sensor measurements. While standard dimension reduction methods such as principal component analysis are often applied to circumvent high-dimensionality, they are often unreliable when the data is discontinuous. This paper presents a ML-based dimension reduction framework to circumvent the challenges of high-dimensional discontinuous machine data. This framework minimizes discontinuity by clustering observations based on the dataset’s modality. The modality is identified using a kernel density estimation parameterized using a heat diffusion solution to the approximate mean integrated squared error. Then, low-dimension representations of each cluster are learned using Laplacian eigenmaps embedding. Finally, the original time sequence of observations across the low-dimensional clusters is used to re-index the observations into a continuous low-dimension feature set. We demonstrate the framework’s utility on common ML-based PdM analysis using the Commercial Modular Aero-Propulsion System Simulation dataset.}
}

@article{bib:COD:linear_algebra,
  author     = {Drineas, Petros and Mahoney, Michael W.},
  title      = {RandNLA: randomized numerical linear algebra},
  year       = {2016},
  issue_date = {June 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {59},
  number     = {6},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/2842602},
  doi        = {10.1145/2842602},
  abstract   = {Randomization offers new benefits for large-scale linear algebra computations.},
  journal    = {Commun. ACM},
  month      = {5},
  pages      = {80–90},
  numpages   = {11}
}

@article{bib:tallest_20,
  title   = {The tallest 20 in 2020: Entering the era of the megatall},
  author  = {Hollister, Nathaniel and Wood, Antony},
  journal = {CTBUH Journal},
  year    = {2012}
}

@article{bib:lsgo_engineering,
  title            = {Layout optimization of large-scale trusses and frames},
  journal          = {Proceedings of IASS Annual Symposia},
  parent_itemid    = {infobike://iass/piass},
  publishercode    = {iass},
  year             = {2018},
  volume           = {2018},
  number           = {19},
  publication_date = {2018-07-16T00:00:00},
  pages            = {1-8},
  itemtype         = {ARTICLE},
  issn             = {2518-6582},
  eissn            = {2518-6582},
  url              = {https://www.ingentaconnect.com/content/iass/piass/2018/00002018/00000019/art00023},
  keyword          = {form finding, layout optimization, stability, minimum weight design},
  author           = {Gilbert, Matthew and He, Linwei and Lu, Hongjia and Tyas, Andy and Fairclough, Helen E and Gondzio, Jacek and Weldeyesus, Alemseged G},
  abstract         = {Layout optimization is a powerful tool that can be used to identify structurally efficient forms for structures such as canopy roofs. With layout optimization a design domain is populated with nodes interconnected by potential members, forming a ground structure. Mathematical optimization techniques can then be used to identify the subset of members forming the optimal (e.g. minimum volume) structure. This contribution provides a brief overview of the layout optimization technique and presents initial findings from a current collaborative research programme designed to expand usage of the technique amongst architects and engineers. In particular, the adaptive member adding solution technique originally developed for simple truss layout optimization problems is shown to also be applicable to problems involving global elastic stability constraints and bending elements. Examples are used to illustrate the range of design problems that can be tackled.}
}

@inproceedings{bib:lsgo_deep_learning,
  author    = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew Y.},
  title     = {On optimization methods for deep learning},
  year      = {2011},
  isbn      = {9781450306195},
  publisher = {Omnipress},
  address   = {Madison, WI, USA},
  abstract  = {The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69\% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining.},
  booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
  pages     = {265–272},
  numpages  = {8},
  location  = {Bellevue, Washington, USA},
  series    = {ICML'11}
}

@article{bib:lsgo_machine_learning,
  author   = {Bottou, L\'{e}on and Curtis, Frank E. and Nocedal, Jorge},
  title    = {Optimization Methods for Large-Scale Machine Learning},
  journal  = {SIAM Review},
  volume   = {60},
  number   = {2},
  pages    = {223-311},
  year     = {2018},
  doi      = {10.1137/16M1080173},
  url      = {https://doi.org/10.1137/16M1080173},
  eprint   = {https://doi.org/10.1137/16M1080173},
  abstract = { This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.}
}

@inproceedings{bib:lsgo_exponential_grouth,
  author    = {Molina, Daniel and Nesterenko, Arthur R. and LaTorre, Antonio},
  booktitle = {2019 IEEE Congress on Evolutionary Computation (CEC)},
  title     = {Comparing Large-Scale Global Optimization Competition winners in a real-world problem},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {359-365},
  keywords  = {Benchmark testing;Optimization;Electroencephalography;Noise measurement;Electrodes;Sociology;Statistics;Large-scale global optimization;LSGO;Benchmarking;BComp},
  doi       = {10.1109/CEC.2019.8789943}
}

@article{bib:swarm_intelligence,
  author         = {Brezočnik, Lucija and Fister, Iztok and Podgorelec, Vili},
  title          = {Swarm Intelligence Algorithms for Feature Selection: A Review},
  journal        = {Applied Sciences},
  volume         = {8},
  year           = {2018},
  number         = {9},
  article-number = {1521},
  issn           = {2076-3417},
  doi            = {10.3390/app8091521},
  url            = {https://www.mdpi.com/2076-3417/8/9/1521},
  abstract       = {The increasingly rapid creation, sharing and exchange of information nowadays put researchers and data scientists ahead of a challenging task of data analysis and extracting relevant information out of data. To be able to learn from data, the dimensionality of the data should be reduced first. Feature selection (FS) can help to reduce the amount of data, but it is a very complex and computationally demanding task, especially in the case of high-dimensional datasets. Swarm intelligence (SI) has been proved as a technique which can solve NP-hard (Non-deterministic Polynomial time) computational problems. It is gaining popularity in solving different optimization problems and has been used successfully for FS in some applications. With the lack of comprehensive surveys in this field, it was our objective to fill the gap in coverage of SI algorithms for FS. We performed a comprehensive literature review of SI algorithms and provide a detailed overview of 64 different SI algorithms for FS, organized into eight major taxonomic categories. We propose a unified SI framework and use it to explain different approaches to FS. Different methods, techniques, and their settings are explained, which have been used for various FS aspects. The datasets used most frequently for the evaluation of SI algorithms for FS are presented, as well as the most common application areas. The guidelines on how to develop SI approaches for FS are provided to support researchers and analysts in their data mining tasks and endeavors while existing issues and open questions are being discussed. In this manner, using the proposed framework and the provided explanations, one should be able to design an SI approach to be used for a specific FS problem.}
}

@inproceedings{bib:si_machine_learning_bad_model,
  author    = {Cheng, Shi and Shi, Yuhui and Qin, Quande and Bai, Ruibin},
  editor    = {Yin, Hujun and Tang, Ke and Gao, Yang and Klawonn, Frank and Lee, Minho and Weise, Thomas and Li, Bin and Yao, Xin},
  title     = {Swarm Intelligence in Big Data Analytics},
  booktitle = {Intelligent Data Engineering and Automated Learning -- IDEAL 2013},
  year      = {2013},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {417--426},
  isbn      = {978-3-642-41278-3},
  abstract  = {This paper analyses the difficulty of big data analytics problems and the potential of swarm intelligence solving big data analytics problems. Nowadays, the big data analytics has attracted more and more attentions, which is required to manage immense amounts of data quickly. However, current researches mainly focus on the amount of data. In this paper, the other three properties of big data analytics, which include the high dimensionality of data, the dynamical change of data, and the multi-objective of problems, are discussed. Swarm intelligence, which works with a population of individuals, is a collection of nature-inspired searching techniques. It has effectively solved many large-scale, dynamical, and multi-objective problems. Based on the combination of swarm intelligence and data mining techniques, we can have better understanding of the big data analytics problems, and designing more effective algorithms to solve real-world big data analytics problems.}
}

@inbook{bib:swarm_intelligence,
  author    = {Kennedy, James},
  editor    = {Zomaya, Albert Y.},
  title     = {Swarm Intelligence},
  booktitle = {Handbook of Nature-Inspired and Innovative Computing: Integrating Classical Models with Emerging Technologies},
  year      = {2006},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {187--219},
  isbn      = {978-0-387-27705-9},
  doi       = {10.1007/0-387-27705-6_6},
  url       = {https://doi.org/10.1007/0-387-27705-6_6}
}

@misc{bib:si_principles,
  title         = {Swarms, Phase Transitions, and Collective Intelligence},
  author        = {Mark M. Millonas},
  year          = {1993},
  eprint        = {adap-org/9306002},
  archiveprefix = {arXiv},
  primaryclass  = {adap-org},
  url           = {https://arxiv.org/pdf/adap-org/9306002}
}

@book{bib:computational_intelligence,
  title     = {Computational intelligence},
  author    = {Kruse, Rudolf and Borgelt, Christian and Braune, Christian and Mostaghim, Sanaz and Steinbrecher, Matthias and Klawonn, Frank and Moewes, Christian},
  year      = {2011},
  publisher = {Springer},
  doi       = {10.1007/978-3-030-42227-1},
  url       = {https://doi.org/10.1007/978-3-030-42227-1},
  isbn      = {978-3-030-42226-4}
}

@article{bib:metaheuristic_lsgo,
  title    = {Metaheuristics in large-scale global continues optimization: A survey},
  journal  = {Information Sciences},
  volume   = {295},
  pages    = {407-428},
  year     = {2015},
  issn     = {0020-0255},
  doi      = {https://doi.org/10.1016/j.ins.2014.10.042},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025514010251},
  author   = {Sedigheh Mahdavi and Mohammad Ebrahim Shiri and Shahryar Rahnamayan},
  keywords = {Large-Scale Global Optimization (LSGO), Evolutionary Algorithms (EAs), Cooperative Coevolution (CC), Problem decomposition, High-dimension, Metaheuristic},
  abstract = {Metaheuristic algorithms are extensively recognized as effective approaches for solving high-dimensional optimization problems. These algorithms provide effective tools with important applications in business, engineering, economics, and science. This paper surveys state-of-the-art metaheuristic algorithms and their current applications in the field of large-scale global optimization. The paper mainly covers the fundamental algorithmic frameworks such as decomposition and non-decomposition methods. More than 200 papers are carefully reviewed to prepare the current comprehensive survey.}
}

@article{bib:psudo_min_cc,
  author   = {van den Bergh, F. and Engelbrecht, A.P.},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {A Cooperative approach to particle swarm optimization},
  year     = {2004},
  volume   = {8},
  number   = {3},
  pages    = {225-239},
  keywords = {Particle swarm optimization;Stochastic processes;Genetic algorithms;Africa;Computer science;Information technology;Neural networks;Topology;Partitioning algorithms;Space technology},
  doi      = {10.1109/TEVC.2004.826069}
}